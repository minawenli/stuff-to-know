{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "military-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-delaware",
   "metadata": {},
   "source": [
    "# Variational Autoencoders\n",
    "\n",
    "The basic setup for VAEs is this:\n",
    "\n",
    "![](resources/vae_picture.png)\n",
    "\n",
    "We parameterise the encoder and decoder as neural networks. We'd like to later use the decoder to sample things (e.g. images of faces), so we need that intermediate space to be something we know how to sample from (e.g. a Gaussian). \n",
    "\n",
    "One way to do this is to just have the output of the encoder be the parameters to a Gaussian:\n",
    "\n",
    "![](resources/reparam_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5)\n",
    "encoder = nn.Linear(5, 2) \n",
    "\n",
    "mu, var = encoder(x) \n",
    "\n",
    "intermediate_representation = normal(mu, var).sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-commons",
   "metadata": {},
   "source": [
    "This intermediate representation is what gets fed into the decoder. \n",
    "\n",
    "The problem is that our network pipeline now looks like this:\n",
    "\n",
    "![](resources/reparam_2.png)\n",
    "\n",
    "The part where we sample the intermediate representation from the normal distribution is not differentiable, so we won't be able to get gradients back to our encoder. \n",
    "\n",
    "To solve this we use the reparameterisation trick. \n",
    "\n",
    "##Â The Reparameterisation Trick \n",
    "\n",
    "The reparameterisation trick is really simple: we just take a sample from a unit Gaussian and add this into our model as an input. Specifically, we sample a value, multiply it by $\\sigma$ and add $\\mu$. \n",
    "\n",
    "I.e. if we have a sample $x$ from a normal distribution with mean 0 and variance 1 then $\\sigma x + \\mu$ will be a sample with mean $\\mu$ and variance $\\sigma^2$.\n",
    "\n",
    "A picture:\n",
    "\n",
    "![](resources/reparam_3.png)\n",
    "\n",
    "Note we still can't backpropagate through sampling, but now we don't need to, the sample is just another input (like the image) to our model. So we could get a gradient for the specific value that we sampled, but we don't need to backpropagate any further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "judicial-salad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0016, -0.3996, -0.2622,  0.3476,  0.6642], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = nn.Linear(5, 2)\n",
    "decoder = nn.Linear(1, 5)\n",
    "\n",
    "x = torch.rand(5)\n",
    "\n",
    "z = encoder(x)\n",
    "h = torch.rand(1)\n",
    "z = (z[1] * h) + z[0] # reparameterisation trick\n",
    "decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-nudist",
   "metadata": {},
   "source": [
    "Now we have a way to do the forward pass, we need a loss function.\n",
    "\n",
    "## The ELBO loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-negotiation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
