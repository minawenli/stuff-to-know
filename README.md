# Topics to know

![GitHub Workflow Status](https://img.shields.io/github/workflow/status/bayeswatch/stuff-to-know/CI)

| All recent methods/ideas a modern machine learner should really know and be able to describe in technical detail        | All neo-classical ( pre 2010 :) ) methods/ideas a modern machine learner should really know and be able to describe in technical detail |
|-------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------|
| Knowledge Distillation                                                                                                  | Principal Component Analysis                                                                                                            |
| Activation Functions - ReLU, Leaky ReLU, PReLU, Random ReLU and other iterations                                        | Support Vector Machines                                                                                                                 |
| Batch Normalisation and normalization in general                                                                        | K-NN                                                                                                                                    |
| Gradient Degradation - What it is and what methods exist to reduce it's effect                                          | K-means, K-center                                                                                                                       |
| GANs - General Working Principle, some uses                                                                             | Gaussian Mixture Models                                                                                                                 |
| Wasserstein Loss                                                                                                        | Random Forests                                                                                                                          |
| Adaptive Optimisers (e.g. Adam)                                                                                         | Boosting                                                                                                                                |
| Transformers - Working principle at a high level, some intuitions, known applications, why are they currently important | Backpropagation                                                                                                                         |
| Relational Networks - General workking principle, intuitions                                                            | SIFT and HOG representations                                                                                                            |
| Comparisons of Relational Network, Convolutional Layer, Fully Connected, Transformer block                              | Visual Bag of words, VLAD, Fisher Vector features                                                                                       |
| Attention - Known types of attention, i.e. hard attention, soft attention, self-attention, and ways to achieve it       | Overfitting - What it is, ways to reduce it                                                                                             |
| ResNet - Skip connections in general - Why are they useful, working principle, applications                             | Generalisation - and we why care                                                                                                        |
| Variational Autoencoders                                                                                                | Bias versus Variance                                                                                                                    |
| Flow-based models                                                                                                       | ~~Gaussian Processes~~                                                                                                                      |
| Meta-Learning - what it is and why we care                                                                              | Graphical Models                                                                                                                        |
| Model Agnostic Meta-Learning - What is all the fuss about gradient based meta-learning and MAML                         | Variational Inference                                                                                                                   |
| Energy based models                                                                                                     | Chain Rule                                                                                                                              |
| Graph Neural Networks                                                                                                   | Fourier Analysis                                                                                                                        |
| Mutual information and information theory                                                                               | Statistical Learning Theory https://www.math.arizona.edu/~hzhang/math574m/Read/vapnik.pdf                                               |
| Reversible and invertible networks                                                                                      | Statistical Hypothesis Testing                                                                                                          |
| Generative flows                                                                                                        | Monte Carlo Inference                                                                                                                   |
| Contrastive learning                                                                                                    | Bayesian Neural Networks                                                                                                                |
| Optimisation types                                                                                                      | Latent Variable Models                                                                                                                  |
| Self supervised learning - What it is, why it's interesting                                                             | Cover's function counting theorem, Probably approximately correct (PAC) learning, PAC-Bayes, VC dimension, Rademacher Complexity        |
| Supervised/Unsupervised learning, basic definitions, pros and cons etc.                                                 | MDPs                                                                                                                                    |
| Neural Architecture Search                                                                                              | Matrix factorisation                                                                                                                    |
|                                                                                                                         | Convex optimisation                                                                                                                     |
|                                                                                                                         | The kernel trick                                                                                                                        |
|                                                                                                                         | Spectral Methods                                                                                                                        |
|                                                                                                                         | Graph Laplcacian, Normalised Graph Laplacian, Laplace-Beltrami Operator, Heat diffusion                                                 |
|                                                                                                                         | Lanczos algorithm (and KPM methods, Chebyshev approximations, Haydock method)                                                           |
|                                                                                                                         | Concentration bounds: Markov Inequality, Chebyshev's inequality, Chernoff/Rubin bounds, Hoeffdingâ€™s inequality                          |
|                                                                                                                         | Johnson lendenstrauss lemma, SVD                                                                                                        |
| |  Precision vs. Recall  |
